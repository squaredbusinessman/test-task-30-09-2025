# Techical specs
Необходимо написать веб-сервис, в который пользователь может отправлять задачи со списком ссылок на файлы из интернета, которые ему нужно скачать. А сервис будет их скачивать и складывать в папку. По созданной задаче можно получать ее статус.

Важное условие: в сервисе должен быть предусмотрен сценарий его перезагрузки/остановки, при этом, насколько это возможно, задачи, которые находятся в это время в обработке (загрузке) необходимо не потерять. Во время такой остановки, в сервис могут поступать новые задачи от пользователей.

Не нужно использовать внешнюю инфраструктуру: докер, БД, nginx и пр. Можно и нужно пользоваться стандартными практиками и паттернами, которые желательно описать в readme.md файле. Если возникают моменты, которые необходимо дополнительно сообщить, опиши их в readme.md

В качестве решения нужно прислать ссылку на репозиторий, который назвать по дате выполнения без упоминания Workmate.

# Downloader Service (Go)

Это небольшой сервис на Go, который принимает задачи на скачивание файлов по URL и складывает их в папку на диске. Никаких внешних БД, докера и прочего — всё максимально простое и приземлённое. Сервис переживает рестарт: незавершённые загрузки не теряются и догружаются после перезапуска (если это поддерживает источник по Range).

## Как запустить

```bash
# из корня проекта
go run ./cmd/server \
  -addr :9090 \
  -data-dir ./tmp/downloads \
  -state-dir ./tmp/state \
  -workers 8
```

Основные параметры можно задавать и через переменные окружения (флаги приоритетнее):

| Env переменная             | Флаг          | Значение по умолчанию |
|----------------------------|---------------|-----------------------|
| `DOWNLOADER_ADDR`          | `-addr`       | `:8080`               |
| `DOWNLOADER_DATA_DIR`      | `-data-dir`   | `./data`              |
| `DOWNLOADER_STATE_DIR`     | `-state-dir`  | `./state`             |
| `DOWNLOADER_WORKERS`       | `-workers`    | `4`                   |

Переменные окружения удобно экспортировать, если конфигурация одна и та же между перезапусками:

```bash
export DOWNLOADER_ADDR=:8000
export DOWNLOADER_WORKERS=2
go run ./cmd/server
```

## Быстрый тест через curl

Создать задачу на скачивание:
```bash
curl -s -X POST http://localhost:8080/tasks \
  -H 'Content-Type: application/json' \
  -d '{"urls":["https://example.com/file1.zip","https://example.com/file2.jpg"]}' | jq .
```
Ответ вернёт id задачи и список частей. Запоминаем `id`.

Статус задачи:
```bash
curl -s http://localhost:8080/tasks/<id> | jq .
```
Список всех задач:
```bash
curl -s http://localhost:8080/tasks | jq .
```
Проверка жизни:
```bash
curl -s http://localhost:8080/health
```

Пример фрагмента задачи:
```json
{
  "id": "a1b2c3d4",
  "created_at": 1710000000,
  "status": "running|done|partial|error",
  "parts": [
    {
      "url": "https://example.com/file1.zip",
      "file_name": "file1.zip",
      "bytes_total": 12345678,
      "bytes_done": 1024,
      "status": "pending|downloading|done|error",
      "error": ""
    }
  ]
}
```

## Как это работает (чуть подробнее)

- При создании задачи сервис раскладывает ссылки по «частям» и сохраняет их состояние в `state/tasks.json` (атомарная запись через tmp+rename).
- Воркеры по очереди скачивают части и периодически обновляют прогресс (байты и статус).
- Если файл уже частично скачан, при возможности продолжим с того же места (HTTP Range). Если сервер Range не поддерживает, придётся качать целиком.
- На рестарте все «висящие» статусы `downloading` переводятся в `pending`, и загрузки продолжаются.

## Почему так, а не иначе

- Без БД. Для задачки с одной нодой JSON-файл достаточен и надёжен, если писать его атомарно.
- Минимум зависимостей. Стандартная библиотека хватает для HTTP и файловых операций.
- Простая очередь. Сейчас масштабирование «по частям» сделано консервативно и прозрачно. Если нужно — несложно распараллелить агрессивнее (например, качать части в нескольких воркерах одновременно).

## Важные детали и ограничения

- Имена файлов берутся из последнего сегмента URL (без query); при совпадении автоматически добавляем суффикс `-{rand}` перед расширением, чтобы не перезаписать уже скачанное.
- Нет ограничений скорости или коннектов. При необходимости добавляем rate limiting и лимиты на домен.
- Нет аутентификации. Предполагается запуск в доверенной среде или за обратным прокси.
- Дедупликации между задачами нет. Можно добавить кеш по контент-хешу.

## Остановка и рестарт

- `Ctrl+C` или `SIGTERM` останавливают HTTP-сервер, воркеры корректно завершают текущие операции, состояние синкается на диск.
- После старта незавершённые задачи автоматически продолжаются.

## Структура кода

- `cmd/server` — входная точка, HTTP и lifecycle
- `internal/api` — HTTP-ручки
- `internal/downloader` — менеджер задач и скачивание, поддержка Range
- `internal/storage` — файловое хранилище состояний (JSON, atomic write)

## Идеи на будущее

- Кеш скачанных артефактов по контент-хешу, чтобы не качать одно и то же.
- Справедливое расписание: лимиты на хост/домен, приоритеты, паузы.
- Чистка старых артефактов по политике (TTL/размер).
- Метрики и структурные логи.

Если где-то оступился или нужен другой интерфейс/формат — напишите, поправлю. Название репозитория рекомендую вида `YYYY-MM-DD`.
